{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running D2C wrapper (therefore computing all possible couples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gpaldino/miniconda3/envs/d2cpy/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pickle \n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, balanced_accuracy_score\n",
    "\n",
    "from d2c.benchmark import D2CWrapper\n",
    "\n",
    "from d2c.descriptors.loader import DataLoader\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "root = './descriptors_old_original/'\n",
    "for file in sorted(os.listdir(root)):\n",
    "    if file.endswith('.pkl'):\n",
    "        df = pd.read_pickle(root + file)\n",
    "        dfs.append(df)\n",
    "\n",
    "data = pd.concat(dfs, axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['process_id', 'graph_id', 'n_variables', 'max_neighborhood_size',\n",
       "       'noise_std', 'edge_source', 'edge_dest', 'is_causal', 'coeff_cause',\n",
       "       'coeff_eff', 'HOC_3_1', 'HOC_1_2', 'HOC_2_1', 'HOC_1_3', 'kurtosis_ca',\n",
       "       'kurtosis_ef', 'mca_mef_cau_q0', 'mca_mef_cau_q1', 'mca_mef_cau_q2',\n",
       "       'mca_mef_cau_q3', 'mca_mef_cau_q4', 'mca_mef_cau_q5', 'mca_mef_cau_q6',\n",
       "       'mca_mef_eff_q0', 'mca_mef_eff_q1', 'mca_mef_eff_q2', 'mca_mef_eff_q3',\n",
       "       'mca_mef_eff_q4', 'mca_mef_eff_q5', 'mca_mef_eff_q6', 'cau_m_eff_q0',\n",
       "       'cau_m_eff_q1', 'cau_m_eff_q2', 'cau_m_eff_q3', 'cau_m_eff_q4',\n",
       "       'cau_m_eff_q5', 'cau_m_eff_q6', 'eff_m_cau_q0', 'eff_m_cau_q1',\n",
       "       'eff_m_cau_q2', 'eff_m_cau_q3', 'eff_m_cau_q4', 'eff_m_cau_q5',\n",
       "       'eff_m_cau_q6', 'm_cau_q0', 'm_cau_q1', 'm_cau_q2', 'm_cau_q3',\n",
       "       'm_cau_q4', 'm_cau_q5', 'm_cau_q6', 'com_cau', 'cau_eff', 'eff_cau',\n",
       "       'eff_cau_mbeff', 'cau_eff_mbcau', 'eff_cau_mbcau_plus_q0',\n",
       "       'eff_cau_mbcau_plus_q1', 'eff_cau_mbcau_plus_q2',\n",
       "       'eff_cau_mbcau_plus_q3', 'eff_cau_mbcau_plus_q4',\n",
       "       'eff_cau_mbcau_plus_q5', 'eff_cau_mbcau_plus_q6',\n",
       "       'cau_eff_mbeff_plus_q0', 'cau_eff_mbeff_plus_q1',\n",
       "       'cau_eff_mbeff_plus_q2', 'cau_eff_mbeff_plus_q3',\n",
       "       'cau_eff_mbeff_plus_q4', 'cau_eff_mbeff_plus_q5',\n",
       "       'cau_eff_mbeff_plus_q6', 'm_eff_q0', 'm_eff_q1', 'm_eff_q2', 'm_eff_q3',\n",
       "       'm_eff_q4', 'm_eff_q5', 'm_eff_q6', 'mca_mca_cau_q0', 'mca_mca_cau_q1',\n",
       "       'mca_mca_cau_q2', 'mca_mca_cau_q3', 'mca_mca_cau_q4', 'mca_mca_cau_q5',\n",
       "       'mca_mca_cau_q6', 'mbe_mbe_eff_q0', 'mbe_mbe_eff_q1', 'mbe_mbe_eff_q2',\n",
       "       'mbe_mbe_eff_q3', 'mbe_mbe_eff_q4', 'mbe_mbe_eff_q5', 'mbe_mbe_eff_q6',\n",
       "       'n_samples', 'n_features', 'n_features/n_samples', 'skewness_ca',\n",
       "       'skewness_ef'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '../data/'\n",
    "\n",
    "# if any exception write it on disk \n",
    "\n",
    "to_dos = []\n",
    "for file in sorted(os.listdir(root)):\n",
    "    if file.endswith('.pkl'):\n",
    "        gen_process_number = int(file.split('_')[0][1:])\n",
    "        n_variables = int(file.split('_')[1][1:])\n",
    "        max_neighborhood_size = int(file.split('_')[2][2:])\n",
    "        noise_std = float(file.split('_')[3][1:-4])\n",
    "        \n",
    "        if noise_std != 0.01:\n",
    "            continue\n",
    "\n",
    "        if max_neighborhood_size != 2:\n",
    "            continue\n",
    "\n",
    "        to_dos.append(file)\n",
    "\n",
    "# sort to_dos by number of variables\n",
    "to_dos_5_variables = [file for file in to_dos if int(file.split('_')[1][1:]) == 5]\n",
    "to_dos_10_variables = [file for file in to_dos if int(file.split('_')[1][1:]) == 10]\n",
    "to_dos_25_variables = [file for file in to_dos if int(file.split('_')[1][1:]) == 25]\n",
    "to_dos_50_variables = [file for file in to_dos if int(file.split('_')[1][1:]) == 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OutEdgeView([(25, 20), (25, 22), (26, 21), (27, 22), (28, 23), (29, 21), (29, 23), (29, 24), (20, 15), (20, 17), (21, 16), (22, 17), (23, 18), (24, 16), (24, 18), (24, 19), (15, 10), (15, 12), (16, 11), (17, 12), (18, 13), (19, 11), (19, 13), (19, 14), (10, 5), (10, 7), (11, 6), (12, 7), (13, 8), (14, 6), (14, 8), (14, 9), (5, 0), (5, 2), (6, 1), (7, 2), (8, 3), (9, 1), (9, 3), (9, 4)])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting a sample of 5 variables\n",
    "file = to_dos_5_variables[0]\n",
    "dataloader = DataLoader(n_variables = n_variables,\n",
    "                maxlags = 5)\n",
    "dataloader.from_pickle(root+file)\n",
    "observations = dataloader.get_original_observations()\n",
    "true_causal_dfs = dataloader.get_true_causal_dfs()\n",
    "dags = dataloader.get_dags()\n",
    "\n",
    "dags[0].edges()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following recomputes all possible pairs with the 'big' d2c!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "destination_root = './d2c_benchmark_old/'\n",
    "#check what is missing from the folder \n",
    "to_do = []\n",
    "for file in to_dos_5_variables+to_dos_10_variables+to_dos_25_variables:\n",
    "    if file not in os.listdir(destination_root):\n",
    "        to_do.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pickle_original_files = set()\n",
    "for file in os.listdir(destination_root):\n",
    "    if file.endswith('.csv'):\n",
    "        pickle_original_file = file.split('_')[0] + '_'+ file.split('_')[1] + '_'+ file.split('_')[2] + '_'+ file.split('_')[3]\n",
    "        pickle_original_files.add(pickle_original_file)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['P4_N25_Nj2_n0.01.pkl',\n",
       " 'P6_N25_Nj2_n0.01.pkl',\n",
       " 'P7_N25_Nj2_n0.01.pkl',\n",
       " 'P3_N25_Nj2_n0.01.pkl',\n",
       " 'P20_N25_Nj2_n0.01.pkl',\n",
       " 'P9_N25_Nj2_n0.01.pkl',\n",
       " 'P8_N25_Nj2_n0.01.pkl',\n",
       " 'P18_N25_Nj2_n0.01.pkl',\n",
       " 'P14_N25_Nj2_n0.01.pkl']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(set(to_dos_5_variables+to_dos_10_variables+to_dos_25_variables).difference(pickle_original_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['P10_N25_Nj2_n0.01.pkl',\n",
       " 'P11_N25_Nj2_n0.01.pkl',\n",
       " 'P12_N25_Nj2_n0.01.pkl',\n",
       " 'P13_N25_Nj2_n0.01.pkl',\n",
       " 'P14_N25_Nj2_n0.01.pkl',\n",
       " 'P15_N25_Nj2_n0.01.pkl',\n",
       " 'P16_N25_Nj2_n0.01.pkl',\n",
       " 'P18_N25_Nj2_n0.01.pkl',\n",
       " 'P19_N25_Nj2_n0.01.pkl',\n",
       " 'P1_N25_Nj2_n0.01.pkl',\n",
       " 'P20_N25_Nj2_n0.01.pkl',\n",
       " 'P2_N25_Nj2_n0.01.pkl',\n",
       " 'P3_N25_Nj2_n0.01.pkl',\n",
       " 'P4_N25_Nj2_n0.01.pkl',\n",
       " 'P6_N25_Nj2_n0.01.pkl',\n",
       " 'P7_N25_Nj2_n0.01.pkl',\n",
       " 'P8_N25_Nj2_n0.01.pkl',\n",
       " 'P9_N25_Nj2_n0.01.pkl']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_dos_25_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [7:09:25<00:00, 2862.88s/it]  \n"
     ]
    }
   ],
   "source": [
    "N_JOBS = 40\n",
    "destination_root = './d2c_benchmark_old/'\n",
    "maxlags = 5\n",
    "for file in tqdm(list(set(to_dos_5_variables+to_dos_10_variables+to_dos_25_variables).difference(pickle_original_files))):\n",
    "    if file.endswith('.pkl'):\n",
    "\n",
    "        #check if the file is already processed\n",
    "        for file_csv in os.listdir(destination_root):\n",
    "            if file_csv.endswith('.csv'):\n",
    "                pickle_original_file = file_csv.split('_')[0] + '_'+ file_csv.split('_')[1] + '_'+ file_csv.split('_')[2] + '_'+ file_csv.split('_')[3]\n",
    "                if pickle_original_file == file:\n",
    "                     continue\n",
    "\n",
    "        gen_process_number = int(file.split('_')[0][1:])\n",
    "        n_variables = int(file.split('_')[1][1:])\n",
    "        max_neighborhood_size = int(file.split('_')[2][2:])\n",
    "        noise_std = float(file.split('_')[3][1:-4])\n",
    "            \n",
    "        filename = f'{destination_root}/P{gen_process_number}_N{n_variables}_Nj{max_neighborhood_size}_n{noise_std}.pkl'\n",
    "\n",
    "        training_data = data.loc[data['process_id'] != gen_process_number]\n",
    "        X_train = training_data.drop(columns=['process_id', 'graph_id', 'n_variables', 'max_neighborhood_size','noise_std', 'edge_source', 'edge_dest', 'is_causal',])\n",
    "        y_train = training_data['is_causal']\n",
    "\n",
    "        model = RandomForestClassifier(n_estimators=50, random_state=0, n_jobs=1)\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        dataloader = DataLoader(n_variables = n_variables,\n",
    "                        maxlags = maxlags)\n",
    "        dataloader.from_pickle(root+file)\n",
    "        observations = dataloader.get_original_observations()\n",
    "        true_causal_dfs = dataloader.get_true_causal_dfs()\n",
    "\n",
    "        d2cwrapper = D2CWrapper(ts_list=observations, \n",
    "                                n_variables=n_variables, \n",
    "                                model=model, \n",
    "                                maxlags=maxlags, \n",
    "                                n_jobs = N_JOBS, \n",
    "                                full=True, \n",
    "                                quantiles=True,\n",
    "                                filename = filename,\n",
    "                                normalize=True, \n",
    "                                cmi='original', \n",
    "                                mb_estimator='original')\n",
    "\n",
    "        d2cwrapper.run()\n",
    "\n",
    "        causal_df = d2cwrapper.get_causal_dfs()\n",
    "\n",
    "        with open(filename, 'wb') as f:\n",
    "                pickle.dump((\n",
    "                            causal_df, \n",
    "                            true_causal_dfs), f)     \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10 vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [11:07:54<00:00, 2109.20s/it] \n"
     ]
    }
   ],
   "source": [
    "root = '../../data/new_data/'\n",
    "destination_root = './'\n",
    "destination = 'd2c_all_couples_MB5_full'\n",
    "if not os.path.exists(destination_root+'/'+destination):\n",
    "    os.makedirs(destination_root+'/'+destination)\n",
    "maxlags = 5\n",
    "# empty folder ../../data/new_benchmark/\n",
    "for todo in [to_dos_10_variables]:\n",
    "    for file in tqdm(todo):\n",
    "        if file.endswith('.pkl'):\n",
    "            gen_process_number = int(file.split('_')[0][1:])\n",
    "            n_variables = int(file.split('_')[1][1:])\n",
    "            max_neighborhood_size = int(file.split('_')[2][2:])\n",
    "            noise_std = float(file.split('_')[3][1:-4])\n",
    "                \n",
    "            filename = f'{destination_root}/{destination}/P{gen_process_number}_N{n_variables}_Nj{max_neighborhood_size}_n{noise_std}.pkl'\n",
    "\n",
    "            training_data = data.loc[data['process_id'] != gen_process_number]\n",
    "            X_train = training_data.drop(columns=['process_id', 'graph_id', 'n_variables', 'max_neighborhood_size','noise_std', 'edge_source', 'edge_dest', 'is_causal',])\n",
    "            y_train = training_data['is_causal']\n",
    "\n",
    "            model = BalancedRandomForestClassifier(n_estimators=50, random_state=0, n_jobs=1, replacement=True, sampling_strategy='all')\n",
    "\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "            dataloader = DataLoader(n_variables = n_variables,\n",
    "                            maxlags = maxlags)\n",
    "            dataloader.from_pickle(root+file)\n",
    "            observations = dataloader.get_original_observations()\n",
    "            true_causal_dfs = dataloader.get_true_causal_dfs()\n",
    "\n",
    "            d2cwrapper = D2CWrapper(ts_list=observations, n_variables=n_variables, model=model, maxlags=maxlags, n_jobs = 55, full=True)\n",
    "\n",
    "            d2cwrapper.run()\n",
    "\n",
    "            causal_df = d2cwrapper.get_causal_dfs()\n",
    "\n",
    "            with open(filename, 'wb') as f:\n",
    "                    pickle.dump((\n",
    "                                causal_df, \n",
    "                                true_causal_dfs), f)     \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "25 vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▎  | 14/19 [15:35:46<5:45:07, 4141.58s/it]"
     ]
    }
   ],
   "source": [
    "root = '../../data/new_data/'\n",
    "destination_root = '../../data/new_benchmark'\n",
    "destination = 'd2c_all_couples_MB5_full'\n",
    "\n",
    "if not os.path.exists(destination_root+'/'+destination):\n",
    "    os.makedirs(destination_root+'/'+destination)\n",
    "maxlags = 5\n",
    "# empty folder ../../data/new_benchmark/\n",
    "\n",
    "for todo in [to_dos_25_variables]:\n",
    "    for file in tqdm(todo):\n",
    "        if file.endswith('.pkl'):\n",
    "            gen_process_number = int(file.split('_')[0][1:])\n",
    "            n_variables = int(file.split('_')[1][1:])\n",
    "            max_neighborhood_size = int(file.split('_')[2][2:])\n",
    "            noise_std = float(file.split('_')[3][1:-4])\n",
    "                \n",
    "            filename = f'{destination_root}/{destination}/P{gen_process_number}_N{n_variables}_Nj{max_neighborhood_size}_n{noise_std}.pkl'\n",
    "\n",
    "            training_data = data.loc[data['process_id'] != gen_process_number]\n",
    "            X_train = training_data.drop(columns=['process_id', 'graph_id', 'n_variables', 'max_neighborhood_size','noise_std', 'edge_source', 'edge_dest', 'is_causal',])\n",
    "            y_train = training_data['is_causal']\n",
    "\n",
    "            model = BalancedRandomForestClassifier(n_estimators=50, random_state=0, n_jobs=1, replacement=True, sampling_strategy='all')\n",
    "\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "            dataloader = DataLoader(n_variables = n_variables,\n",
    "                            maxlags = maxlags)\n",
    "            dataloader.from_pickle(root+file)\n",
    "            observations = dataloader.get_original_observations()\n",
    "            true_causal_dfs = dataloader.get_true_causal_dfs()\n",
    "\n",
    "\n",
    "            d2cwrapper = D2CWrapper(ts_list=observations, n_variables=n_variables, model=model, maxlags=maxlags, n_jobs = 55, full=True)\n",
    "\n",
    "            d2cwrapper.run()\n",
    "\n",
    "            causal_df = d2cwrapper.get_causal_dfs()\n",
    "\n",
    "            with open(filename, 'wb') as f:\n",
    "                    pickle.dump((\n",
    "                                causal_df, \n",
    "                                true_causal_dfs), f)     \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [2:07:35<00:00, 3827.78s/it]  \n"
     ]
    }
   ],
   "source": [
    "# handle the two missing files separately\n",
    "missing_files = ['P8_N25_Nj8_n0.005.pkl','P9_N25_Nj8_n0.005.pkl']\n",
    "\n",
    "root = '../../data/new_data/'\n",
    "destination_root = '../../data/new_benchmark'\n",
    "destination = 'd2c_all_couples_MB5_full'\n",
    "\n",
    "maxlags = 5\n",
    "# empty folder ../../data/new_benchmark/\n",
    "\n",
    "\n",
    "for file in tqdm(missing_files):\n",
    "    if file.endswith('.pkl'):\n",
    "        gen_process_number = int(file.split('_')[0][1:])\n",
    "        n_variables = int(file.split('_')[1][1:])\n",
    "        max_neighborhood_size = int(file.split('_')[2][2:])\n",
    "        noise_std = float(file.split('_')[3][1:-4])\n",
    "            \n",
    "        filename = f'{destination_root}/{destination}/P{gen_process_number}_N{n_variables}_Nj{max_neighborhood_size}_n{noise_std}.pkl'\n",
    "\n",
    "        training_data = data.loc[data['process_id'] != gen_process_number]\n",
    "        X_train = training_data.drop(columns=['process_id', 'graph_id', 'n_variables', 'max_neighborhood_size','noise_std', 'edge_source', 'edge_dest', 'is_causal',])\n",
    "        y_train = training_data['is_causal']\n",
    "\n",
    "        model = BalancedRandomForestClassifier(n_estimators=50, random_state=0, n_jobs=1, replacement=True, sampling_strategy='all')\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        dataloader = DataLoader(n_variables = n_variables,\n",
    "                        maxlags = maxlags)\n",
    "        dataloader.from_pickle(root+file)\n",
    "        observations = dataloader.get_original_observations()\n",
    "        true_causal_dfs = dataloader.get_true_causal_dfs()\n",
    "\n",
    "\n",
    "        d2cwrapper = D2CWrapper(ts_list=observations, n_variables=n_variables, model=model, maxlags=maxlags, n_jobs = 55, full=True)\n",
    "\n",
    "        d2cwrapper.run()\n",
    "\n",
    "        causal_df = d2cwrapper.get_causal_dfs()\n",
    "\n",
    "        with open(filename, 'wb') as f:\n",
    "                pickle.dump((\n",
    "                            causal_df, \n",
    "                            true_causal_dfs), f)     \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "50 variables is just too much"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [1:39:35<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/gpaldino/D2CPY/notebooks/new_notebooks/03_run_d2c_all_couples.ipynb Cell 12\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B51.178.93.50/home/gpaldino/D2CPY/notebooks/new_notebooks/03_run_d2c_all_couples.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m true_causal_dfs \u001b[39m=\u001b[39m dataloader\u001b[39m.\u001b[39mget_true_causal_dfs()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B51.178.93.50/home/gpaldino/D2CPY/notebooks/new_notebooks/03_run_d2c_all_couples.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=33'>34</a>\u001b[0m d2cwrapper \u001b[39m=\u001b[39m D2CWrapper(ts_list\u001b[39m=\u001b[39mobservations, n_variables\u001b[39m=\u001b[39mn_variables, model\u001b[39m=\u001b[39mmodel, maxlags\u001b[39m=\u001b[39mmaxlags, n_jobs \u001b[39m=\u001b[39m \u001b[39m55\u001b[39m, full\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B51.178.93.50/home/gpaldino/D2CPY/notebooks/new_notebooks/03_run_d2c_all_couples.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=35'>36</a>\u001b[0m d2cwrapper\u001b[39m.\u001b[39;49mrun()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B51.178.93.50/home/gpaldino/D2CPY/notebooks/new_notebooks/03_run_d2c_all_couples.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=37'>38</a>\u001b[0m causal_df \u001b[39m=\u001b[39m d2cwrapper\u001b[39m.\u001b[39mget_causal_dfs()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B51.178.93.50/home/gpaldino/D2CPY/notebooks/new_notebooks/03_run_d2c_all_couples.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=39'>40</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(filename, \u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n",
      "File \u001b[0;32m~/D2CPY/src/d2c/benchmark/base.py:52\u001b[0m, in \u001b[0;36mBaseCausalInference.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m     \u001b[39mwith\u001b[39;00m Pool(processes\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_jobs) \u001b[39mas\u001b[39;00m pool:\n\u001b[0;32m---> 52\u001b[0m         results \u001b[39m=\u001b[39m pool\u001b[39m.\u001b[39;49mmap(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprocess_ts, ts_tuples)\n\u001b[1;32m     53\u001b[0m     \u001b[39mfor\u001b[39;00m ts_index, causal_df \u001b[39min\u001b[39;00m results:\n\u001b[1;32m     54\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcausal_dfs[ts_index] \u001b[39m=\u001b[39m causal_df\n",
      "File \u001b[0;32m~/miniconda3/envs/d2cpy/lib/python3.8/multiprocessing/pool.py:364\u001b[0m, in \u001b[0;36mPool.map\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmap\u001b[39m(\u001b[39mself\u001b[39m, func, iterable, chunksize\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    360\u001b[0m \u001b[39m    \u001b[39m\u001b[39m'''\u001b[39;00m\n\u001b[1;32m    361\u001b[0m \u001b[39m    Apply `func` to each element in `iterable`, collecting the results\u001b[39;00m\n\u001b[1;32m    362\u001b[0m \u001b[39m    in a list that is returned.\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[39m    '''\u001b[39;00m\n\u001b[0;32m--> 364\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_map_async(func, iterable, mapstar, chunksize)\u001b[39m.\u001b[39;49mget()\n",
      "File \u001b[0;32m~/miniconda3/envs/d2cpy/lib/python3.8/multiprocessing/pool.py:765\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget\u001b[39m(\u001b[39mself\u001b[39m, timeout\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 765\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    766\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mready():\n\u001b[1;32m    767\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/d2cpy/lib/python3.8/multiprocessing/pool.py:762\u001b[0m, in \u001b[0;36mApplyResult.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwait\u001b[39m(\u001b[39mself\u001b[39m, timeout\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 762\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_event\u001b[39m.\u001b[39;49mwait(timeout)\n",
      "File \u001b[0;32m~/miniconda3/envs/d2cpy/lib/python3.8/threading.py:558\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    556\u001b[0m signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flag\n\u001b[1;32m    557\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 558\u001b[0m     signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cond\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    559\u001b[0m \u001b[39mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m~/miniconda3/envs/d2cpy/lib/python3.8/threading.py:302\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    301\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 302\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[1;32m    303\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    304\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# root = '../../data/new_data/'\n",
    "# destination_root = '../../data/new_benchmark'\n",
    "# destination = 'd2c_all_couples_MB5_full'\n",
    "\n",
    "# if not os.path.exists(destination_root+'/'+destination):\n",
    "#     os.makedirs(destination_root+'/'+destination)\n",
    "# maxlags = 5\n",
    "# # empty folder ../../data/new_benchmark/\n",
    "\n",
    "# for todo in [to_dos_50_variables]:\n",
    "#     for file in tqdm(todo):\n",
    "#         if file.endswith('.pkl'):\n",
    "#             gen_process_number = int(file.split('_')[0][1:])\n",
    "#             n_variables = int(file.split('_')[1][1:])\n",
    "#             max_neighborhood_size = int(file.split('_')[2][2:])\n",
    "#             noise_std = float(file.split('_')[3][1:-4])\n",
    "                \n",
    "#             filename = f'{destination_root}/{destination}/P{gen_process_number}_N{n_variables}_Nj{max_neighborhood_size}_n{noise_std}.pkl'\n",
    "\n",
    "#             training_data = data.loc[data['process_id'] != gen_process_number]\n",
    "#             X_train = training_data.drop(columns=['process_id', 'graph_id', 'n_variables', 'max_neighborhood_size','noise_std', 'edge_source', 'edge_dest', 'is_causal',])\n",
    "#             y_train = training_data['is_causal']\n",
    "\n",
    "#             model = BalancedRandomForestClassifier(n_estimators=50, random_state=0, n_jobs=1, replacement=True, sampling_strategy='all')\n",
    "\n",
    "#             model.fit(X_train, y_train)\n",
    "\n",
    "#             dataloader = DataLoader(n_variables = n_variables,\n",
    "#                             maxlags = maxlags)\n",
    "#             dataloader.from_pickle(root+file)\n",
    "#             observations = dataloader.get_original_observations()\n",
    "#             true_causal_dfs = dataloader.get_true_causal_dfs()\n",
    "\n",
    "\n",
    "#             d2cwrapper = D2CWrapper(ts_list=observations, n_variables=n_variables, model=model, maxlags=maxlags, n_jobs = 55, full=True)\n",
    "\n",
    "#             d2cwrapper.run()\n",
    "\n",
    "#             causal_df = d2cwrapper.get_causal_dfs()\n",
    "\n",
    "#             with open(filename, 'wb') as f:\n",
    "#                     pickle.dump((\n",
    "#                                 causal_df, \n",
    "#                                 true_causal_dfs), f)     \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2cpy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
