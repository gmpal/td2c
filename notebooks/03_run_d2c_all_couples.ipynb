{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running D2C wrapper (therefore computing all possible couples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, balanced_accuracy_score\n",
    "\n",
    "from d2c.benchmark import D2CWrapper\n",
    "\n",
    "from d2c.descriptors.loader import DataLoader\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "root = './descriptors/'\n",
    "for file in sorted(os.listdir(root)):\n",
    "    if file.endswith('.pkl'):\n",
    "        df = pd.read_pickle(root + file)\n",
    "        dfs.append(df)\n",
    "\n",
    "data = pd.concat(dfs, axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = './data/'\n",
    "\n",
    "# if any exception write it on disk \n",
    "\n",
    "to_dos = []\n",
    "for file in sorted(os.listdir(root)):\n",
    "    if file.endswith('.pkl'):\n",
    "        gen_process_number = int(file.split('_')[0][1:])\n",
    "        n_variables = int(file.split('_')[1][1:])\n",
    "        max_neighborhood_size = int(file.split('_')[2][2:])\n",
    "        noise_std = float(file.split('_')[3][1:-4])\n",
    "        \n",
    "        if noise_std != 0.005:\n",
    "            continue\n",
    "\n",
    "        if max_neighborhood_size != 8:\n",
    "            continue\n",
    "\n",
    "        to_dos.append(file)\n",
    "\n",
    "# sort to_dos by number of variables\n",
    "to_dos_5_variables = [file for file in to_dos if int(file.split('_')[1][1:]) == 5]\n",
    "to_dos_10_variables = [file for file in to_dos if int(file.split('_')[1][1:]) == 10]\n",
    "to_dos_25_variables = [file for file in to_dos if int(file.split('_')[1][1:]) == 25]\n",
    "to_dos_50_variables = [file for file in to_dos if int(file.split('_')[1][1:]) == 50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following recomputes all possible pairs with the 'big' d2c!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in to_dos_25_variables+to_dos_10_variables+to_dos_5_variables:\n",
    "    if file not in os.listdir( './d2c_benchmark/'):\n",
    "        print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [7:15:06<00:00, 4351.01s/it]  \n"
     ]
    }
   ],
   "source": [
    "root = './data/'\n",
    "destination_root = './d2c_benchmark/'\n",
    "maxlags = 5\n",
    "for file in tqdm(todo):\n",
    "    if file.endswith('.pkl'):\n",
    "        gen_process_number = int(file.split('_')[0][1:])\n",
    "        n_variables = int(file.split('_')[1][1:])\n",
    "        max_neighborhood_size = int(file.split('_')[2][2:])\n",
    "        noise_std = float(file.split('_')[3][1:-4])\n",
    "            \n",
    "        filename = f'{destination_root}/P{gen_process_number}_N{n_variables}_Nj{max_neighborhood_size}_n{noise_std}.pkl'\n",
    "\n",
    "        training_data = data.loc[data['process_id'] != gen_process_number]\n",
    "        X_train = training_data.drop(columns=['process_id', 'graph_id', 'n_variables', 'max_neighborhood_size','noise_std', 'edge_source', 'edge_dest', 'is_causal',])\n",
    "        y_train = training_data['is_causal']\n",
    "\n",
    "        model = RandomForestClassifier(n_estimators=50, random_state=0, n_jobs=1)\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        dataloader = DataLoader(n_variables = n_variables,\n",
    "                        maxlags = maxlags)\n",
    "        dataloader.from_pickle(root+file)\n",
    "        observations = dataloader.get_original_observations()\n",
    "        true_causal_dfs = dataloader.get_true_causal_dfs()\n",
    "\n",
    "        d2cwrapper = D2CWrapper(ts_list=observations, n_variables=n_variables, model=model, maxlags=maxlags, n_jobs = 40, full=True)\n",
    "\n",
    "        d2cwrapper.run()\n",
    "\n",
    "        causal_df = d2cwrapper.get_causal_dfs()\n",
    "\n",
    "        with open(filename, 'wb') as f:\n",
    "                pickle.dump((\n",
    "                            causal_df, \n",
    "                            true_causal_dfs), f)     \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10 vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [11:07:54<00:00, 2109.20s/it] \n"
     ]
    }
   ],
   "source": [
    "root = '../../data/new_data/'\n",
    "destination_root = '../../data/new_benchmark'\n",
    "destination = 'd2c_all_couples_MB5_full'\n",
    "if not os.path.exists(destination_root+'/'+destination):\n",
    "    os.makedirs(destination_root+'/'+destination)\n",
    "maxlags = 5\n",
    "# empty folder ../../data/new_benchmark/\n",
    "for todo in [to_dos_10_variables]:\n",
    "    for file in tqdm(todo):\n",
    "        if file.endswith('.pkl'):\n",
    "            gen_process_number = int(file.split('_')[0][1:])\n",
    "            n_variables = int(file.split('_')[1][1:])\n",
    "            max_neighborhood_size = int(file.split('_')[2][2:])\n",
    "            noise_std = float(file.split('_')[3][1:-4])\n",
    "                \n",
    "            filename = f'{destination_root}/{destination}/P{gen_process_number}_N{n_variables}_Nj{max_neighborhood_size}_n{noise_std}.pkl'\n",
    "\n",
    "            training_data = data.loc[data['process_id'] != gen_process_number]\n",
    "            X_train = training_data.drop(columns=['process_id', 'graph_id', 'n_variables', 'max_neighborhood_size','noise_std', 'edge_source', 'edge_dest', 'is_causal',])\n",
    "            y_train = training_data['is_causal']\n",
    "\n",
    "            model = BalancedRandomForestClassifier(n_estimators=50, random_state=0, n_jobs=1, replacement=True, sampling_strategy='all')\n",
    "\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "            dataloader = DataLoader(n_variables = n_variables,\n",
    "                            maxlags = maxlags)\n",
    "            dataloader.from_pickle(root+file)\n",
    "            observations = dataloader.get_original_observations()\n",
    "            true_causal_dfs = dataloader.get_true_causal_dfs()\n",
    "\n",
    "            d2cwrapper = D2CWrapper(ts_list=observations, n_variables=n_variables, model=model, maxlags=maxlags, n_jobs = 55, full=True)\n",
    "\n",
    "            d2cwrapper.run()\n",
    "\n",
    "            causal_df = d2cwrapper.get_causal_dfs()\n",
    "\n",
    "            with open(filename, 'wb') as f:\n",
    "                    pickle.dump((\n",
    "                                causal_df, \n",
    "                                true_causal_dfs), f)     \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "25 vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▎  | 14/19 [15:35:46<5:45:07, 4141.58s/it]"
     ]
    }
   ],
   "source": [
    "root = '../../data/new_data/'\n",
    "destination_root = '../../data/new_benchmark'\n",
    "destination = 'd2c_all_couples_MB5_full'\n",
    "\n",
    "if not os.path.exists(destination_root+'/'+destination):\n",
    "    os.makedirs(destination_root+'/'+destination)\n",
    "maxlags = 5\n",
    "# empty folder ../../data/new_benchmark/\n",
    "\n",
    "for todo in [to_dos_25_variables]:\n",
    "    for file in tqdm(todo):\n",
    "        if file.endswith('.pkl'):\n",
    "            gen_process_number = int(file.split('_')[0][1:])\n",
    "            n_variables = int(file.split('_')[1][1:])\n",
    "            max_neighborhood_size = int(file.split('_')[2][2:])\n",
    "            noise_std = float(file.split('_')[3][1:-4])\n",
    "                \n",
    "            filename = f'{destination_root}/{destination}/P{gen_process_number}_N{n_variables}_Nj{max_neighborhood_size}_n{noise_std}.pkl'\n",
    "\n",
    "            training_data = data.loc[data['process_id'] != gen_process_number]\n",
    "            X_train = training_data.drop(columns=['process_id', 'graph_id', 'n_variables', 'max_neighborhood_size','noise_std', 'edge_source', 'edge_dest', 'is_causal',])\n",
    "            y_train = training_data['is_causal']\n",
    "\n",
    "            model = BalancedRandomForestClassifier(n_estimators=50, random_state=0, n_jobs=1, replacement=True, sampling_strategy='all')\n",
    "\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "            dataloader = DataLoader(n_variables = n_variables,\n",
    "                            maxlags = maxlags)\n",
    "            dataloader.from_pickle(root+file)\n",
    "            observations = dataloader.get_original_observations()\n",
    "            true_causal_dfs = dataloader.get_true_causal_dfs()\n",
    "\n",
    "\n",
    "            d2cwrapper = D2CWrapper(ts_list=observations, n_variables=n_variables, model=model, maxlags=maxlags, n_jobs = 55, full=True)\n",
    "\n",
    "            d2cwrapper.run()\n",
    "\n",
    "            causal_df = d2cwrapper.get_causal_dfs()\n",
    "\n",
    "            with open(filename, 'wb') as f:\n",
    "                    pickle.dump((\n",
    "                                causal_df, \n",
    "                                true_causal_dfs), f)     \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [2:07:35<00:00, 3827.78s/it]  \n"
     ]
    }
   ],
   "source": [
    "# handle the two missing files separately\n",
    "missing_files = ['P8_N25_Nj8_n0.005.pkl','P9_N25_Nj8_n0.005.pkl']\n",
    "\n",
    "root = '../../data/new_data/'\n",
    "destination_root = '../../data/new_benchmark'\n",
    "destination = 'd2c_all_couples_MB5_full'\n",
    "\n",
    "maxlags = 5\n",
    "# empty folder ../../data/new_benchmark/\n",
    "\n",
    "\n",
    "for file in tqdm(missing_files):\n",
    "    if file.endswith('.pkl'):\n",
    "        gen_process_number = int(file.split('_')[0][1:])\n",
    "        n_variables = int(file.split('_')[1][1:])\n",
    "        max_neighborhood_size = int(file.split('_')[2][2:])\n",
    "        noise_std = float(file.split('_')[3][1:-4])\n",
    "            \n",
    "        filename = f'{destination_root}/{destination}/P{gen_process_number}_N{n_variables}_Nj{max_neighborhood_size}_n{noise_std}.pkl'\n",
    "\n",
    "        training_data = data.loc[data['process_id'] != gen_process_number]\n",
    "        X_train = training_data.drop(columns=['process_id', 'graph_id', 'n_variables', 'max_neighborhood_size','noise_std', 'edge_source', 'edge_dest', 'is_causal',])\n",
    "        y_train = training_data['is_causal']\n",
    "\n",
    "        model = BalancedRandomForestClassifier(n_estimators=50, random_state=0, n_jobs=1, replacement=True, sampling_strategy='all')\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        dataloader = DataLoader(n_variables = n_variables,\n",
    "                        maxlags = maxlags)\n",
    "        dataloader.from_pickle(root+file)\n",
    "        observations = dataloader.get_original_observations()\n",
    "        true_causal_dfs = dataloader.get_true_causal_dfs()\n",
    "\n",
    "\n",
    "        d2cwrapper = D2CWrapper(ts_list=observations, n_variables=n_variables, model=model, maxlags=maxlags, n_jobs = 55, full=True)\n",
    "\n",
    "        d2cwrapper.run()\n",
    "\n",
    "        causal_df = d2cwrapper.get_causal_dfs()\n",
    "\n",
    "        with open(filename, 'wb') as f:\n",
    "                pickle.dump((\n",
    "                            causal_df, \n",
    "                            true_causal_dfs), f)     \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "50 variables is just too much"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [1:39:35<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/gpaldino/D2CPY/notebooks/new_notebooks/03_run_d2c_all_couples.ipynb Cell 12\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B51.178.93.50/home/gpaldino/D2CPY/notebooks/new_notebooks/03_run_d2c_all_couples.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m true_causal_dfs \u001b[39m=\u001b[39m dataloader\u001b[39m.\u001b[39mget_true_causal_dfs()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B51.178.93.50/home/gpaldino/D2CPY/notebooks/new_notebooks/03_run_d2c_all_couples.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=33'>34</a>\u001b[0m d2cwrapper \u001b[39m=\u001b[39m D2CWrapper(ts_list\u001b[39m=\u001b[39mobservations, n_variables\u001b[39m=\u001b[39mn_variables, model\u001b[39m=\u001b[39mmodel, maxlags\u001b[39m=\u001b[39mmaxlags, n_jobs \u001b[39m=\u001b[39m \u001b[39m55\u001b[39m, full\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B51.178.93.50/home/gpaldino/D2CPY/notebooks/new_notebooks/03_run_d2c_all_couples.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=35'>36</a>\u001b[0m d2cwrapper\u001b[39m.\u001b[39;49mrun()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B51.178.93.50/home/gpaldino/D2CPY/notebooks/new_notebooks/03_run_d2c_all_couples.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=37'>38</a>\u001b[0m causal_df \u001b[39m=\u001b[39m d2cwrapper\u001b[39m.\u001b[39mget_causal_dfs()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B51.178.93.50/home/gpaldino/D2CPY/notebooks/new_notebooks/03_run_d2c_all_couples.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=39'>40</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(filename, \u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n",
      "File \u001b[0;32m~/D2CPY/src/d2c/benchmark/base.py:52\u001b[0m, in \u001b[0;36mBaseCausalInference.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m     \u001b[39mwith\u001b[39;00m Pool(processes\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_jobs) \u001b[39mas\u001b[39;00m pool:\n\u001b[0;32m---> 52\u001b[0m         results \u001b[39m=\u001b[39m pool\u001b[39m.\u001b[39;49mmap(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprocess_ts, ts_tuples)\n\u001b[1;32m     53\u001b[0m     \u001b[39mfor\u001b[39;00m ts_index, causal_df \u001b[39min\u001b[39;00m results:\n\u001b[1;32m     54\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcausal_dfs[ts_index] \u001b[39m=\u001b[39m causal_df\n",
      "File \u001b[0;32m~/miniconda3/envs/d2cpy/lib/python3.8/multiprocessing/pool.py:364\u001b[0m, in \u001b[0;36mPool.map\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmap\u001b[39m(\u001b[39mself\u001b[39m, func, iterable, chunksize\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    360\u001b[0m \u001b[39m    \u001b[39m\u001b[39m'''\u001b[39;00m\n\u001b[1;32m    361\u001b[0m \u001b[39m    Apply `func` to each element in `iterable`, collecting the results\u001b[39;00m\n\u001b[1;32m    362\u001b[0m \u001b[39m    in a list that is returned.\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[39m    '''\u001b[39;00m\n\u001b[0;32m--> 364\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_map_async(func, iterable, mapstar, chunksize)\u001b[39m.\u001b[39;49mget()\n",
      "File \u001b[0;32m~/miniconda3/envs/d2cpy/lib/python3.8/multiprocessing/pool.py:765\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget\u001b[39m(\u001b[39mself\u001b[39m, timeout\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 765\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    766\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mready():\n\u001b[1;32m    767\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/d2cpy/lib/python3.8/multiprocessing/pool.py:762\u001b[0m, in \u001b[0;36mApplyResult.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwait\u001b[39m(\u001b[39mself\u001b[39m, timeout\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 762\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_event\u001b[39m.\u001b[39;49mwait(timeout)\n",
      "File \u001b[0;32m~/miniconda3/envs/d2cpy/lib/python3.8/threading.py:558\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    556\u001b[0m signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flag\n\u001b[1;32m    557\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 558\u001b[0m     signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cond\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    559\u001b[0m \u001b[39mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m~/miniconda3/envs/d2cpy/lib/python3.8/threading.py:302\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    301\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 302\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[1;32m    303\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    304\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# root = '../../data/new_data/'\n",
    "# destination_root = '../../data/new_benchmark'\n",
    "# destination = 'd2c_all_couples_MB5_full'\n",
    "\n",
    "# if not os.path.exists(destination_root+'/'+destination):\n",
    "#     os.makedirs(destination_root+'/'+destination)\n",
    "# maxlags = 5\n",
    "# # empty folder ../../data/new_benchmark/\n",
    "\n",
    "# for todo in [to_dos_50_variables]:\n",
    "#     for file in tqdm(todo):\n",
    "#         if file.endswith('.pkl'):\n",
    "#             gen_process_number = int(file.split('_')[0][1:])\n",
    "#             n_variables = int(file.split('_')[1][1:])\n",
    "#             max_neighborhood_size = int(file.split('_')[2][2:])\n",
    "#             noise_std = float(file.split('_')[3][1:-4])\n",
    "                \n",
    "#             filename = f'{destination_root}/{destination}/P{gen_process_number}_N{n_variables}_Nj{max_neighborhood_size}_n{noise_std}.pkl'\n",
    "\n",
    "#             training_data = data.loc[data['process_id'] != gen_process_number]\n",
    "#             X_train = training_data.drop(columns=['process_id', 'graph_id', 'n_variables', 'max_neighborhood_size','noise_std', 'edge_source', 'edge_dest', 'is_causal',])\n",
    "#             y_train = training_data['is_causal']\n",
    "\n",
    "#             model = BalancedRandomForestClassifier(n_estimators=50, random_state=0, n_jobs=1, replacement=True, sampling_strategy='all')\n",
    "\n",
    "#             model.fit(X_train, y_train)\n",
    "\n",
    "#             dataloader = DataLoader(n_variables = n_variables,\n",
    "#                             maxlags = maxlags)\n",
    "#             dataloader.from_pickle(root+file)\n",
    "#             observations = dataloader.get_original_observations()\n",
    "#             true_causal_dfs = dataloader.get_true_causal_dfs()\n",
    "\n",
    "\n",
    "#             d2cwrapper = D2CWrapper(ts_list=observations, n_variables=n_variables, model=model, maxlags=maxlags, n_jobs = 55, full=True)\n",
    "\n",
    "#             d2cwrapper.run()\n",
    "\n",
    "#             causal_df = d2cwrapper.get_causal_dfs()\n",
    "\n",
    "#             with open(filename, 'wb') as f:\n",
    "#                     pickle.dump((\n",
    "#                                 causal_df, \n",
    "#                                 true_causal_dfs), f)     \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2cpy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
