{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv  # You can also use GATConv, GraphSAGE, etc.\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_47113/4129765986.py:1: DtypeWarning: Columns (92) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  descriptors = pd.read_csv('example/descriptors.csv')\n"
     ]
    }
   ],
   "source": [
    "descriptors = pd.read_csv('example/descriptors.csv')\n",
    "\n",
    "descriptors.head()\n",
    "\n",
    "node_labels = descriptors.edge_source.unique()\n",
    "node_labels_mapping = {label: i for i, label in enumerate(node_labels)}\n",
    "\n",
    "descriptors['edge_source'] = descriptors['edge_source'].map(node_labels_mapping)\n",
    "descriptors['edge_dest'] = descriptors['edge_dest'].map(node_labels_mapping)\n",
    "\n",
    "to_drop = [\n",
    "    \"function_id\",\n",
    "    \"graph_id\",\n",
    "    \"edge_source\",\n",
    "    \"edge_dest\",\n",
    "    \"is_causal\",\n",
    "]\n",
    "\n",
    "features = descriptors.columns.difference(to_drop)\n",
    "\n",
    "features = ['coeff_cause', 'kurtosis_ef', 'kurtosis_ca', 'HOC_1_3', 'coeff_eff',\n",
    "       'HOC_3_1', 'eff_m_cau_q0', 'eff_m_cau_q1', 'eff_m_cau_q5',\n",
    "       'skewness_ef', 'eff_m_cau_q6', 'm_eff_q5', 'eff_m_cau_q3',\n",
    "       'skewness_ca', 'eff_m_cau_q2', 'eff_m_cau_q4', 'HOC_2_1', 'm_eff_q6',\n",
    "       'HOC_1_2', 'm_eff_q0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_data_list = []\n",
    "for graph_id in descriptors.graph_id.unique(): \n",
    "    single_graph = descriptors.loc[descriptors.graph_id == graph_id]\n",
    "\n",
    "    stacked_edges = np.vstack([single_graph.edge_source.values,\n",
    "         single_graph.edge_dest.values])\n",
    "    \n",
    "    data = Data(\n",
    "        x=torch.ones((len(single_graph.edge_source.unique()), 1)),\n",
    "        edge_index=torch.tensor(stacked_edges, dtype=torch.long),\n",
    "        edge_attr=torch.tensor(single_graph[features].values, dtype=torch.float),\n",
    "        y=torch.tensor(single_graph.is_causal.values, dtype=torch.float).unsqueeze(1)\n",
    "    )\n",
    "    graph_data_list.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = len(node_labels)\n",
    "node_features = 1\n",
    "edge_features = len(features)\n",
    "num_edges = single_graph.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Dataset\n",
    "\n",
    "class CustomGraphDataset(Dataset):\n",
    "    def __init__(self, data_list):\n",
    "        super(CustomGraphDataset, self).__init__()\n",
    "        self.data_list = data_list\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def get(self, idx):\n",
    "        return self.data_list[idx]\n",
    "\n",
    "# Create the dataset from your list of Data objects\n",
    "dataset = CustomGraphDataset(graph_data_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "\n",
    "class EdgeClassifier(torch.nn.Module):\n",
    "    def __init__(self, node_feature_dim, edge_feature_dim, hidden_dim):\n",
    "        super(EdgeClassifier, self).__init__()\n",
    "        self.conv1 = GCNConv(node_feature_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.edge_mlp = torch.nn.Sequential(\n",
    "            torch.nn.Linear(2 * hidden_dim + edge_feature_dim, hidden_dim),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden_dim, 1)  # Assuming binary classification\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index, batch, edge_attr):\n",
    "        # Node feature learning\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = self.conv2(x, edge_index)\n",
    "\n",
    "        # Edge feature concatenation (node features of source and target + original edge features)\n",
    "        row, col = edge_index\n",
    "        edge_feat = torch.cat([x[row], x[col], edge_attr], dim=1)\n",
    "\n",
    "        # Use batch information if necessary, for example to aggregate graph-level features\n",
    "        x =  global_mean_pool(x, batch)  # Uncomment if node-level to graph-level is needed\n",
    "\n",
    "        # Edge classification\n",
    "        return torch.sigmoid(self.edge_mlp(edge_feat))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "# Split the dataset\n",
    "train_size = int(0.7 * len(dataset))\n",
    "val_size = int(0.15 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# Data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume some dimensions for the example\n",
    "hidden_dim = 16\n",
    "\n",
    "model = EdgeClassifier(node_features, edge_features, hidden_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 1.9089626974218032, Validation Loss: 1.6971874175752912\n",
      "Epoch 2: Train Loss: 1.8008706800476844, Validation Loss: 1.740078119096302\n",
      "Epoch 3: Train Loss: 1.9552711681959007, Validation Loss: 1.5376230993725004\n",
      "Epoch 4: Train Loss: 1.679688756125314, Validation Loss: 1.5689415339061192\n",
      "Epoch 5: Train Loss: 1.676900782705355, Validation Loss: 1.5695895108722504\n",
      "Epoch 6: Train Loss: 1.7014341125167718, Validation Loss: 1.7155283298946562\n",
      "Epoch 7: Train Loss: 1.7577015348642815, Validation Loss: 1.6734077803293863\n",
      "Epoch 8: Train Loss: 1.6856637951105582, Validation Loss: 1.512958423183078\n",
      "Epoch 9: Train Loss: 1.739669939890629, Validation Loss: 1.5912588914235433\n",
      "Epoch 10: Train Loss: 1.6420836867805289, Validation Loss: 1.6254246125902447\n",
      "Epoch 11: Train Loss: 1.7560079332159346, Validation Loss: 1.707205636614845\n",
      "Epoch 12: Train Loss: 1.8575716170543382, Validation Loss: 1.6877021076565697\n",
      "Epoch 13: Train Loss: 1.6730247097255804, Validation Loss: 1.5403233687082927\n",
      "Epoch 14: Train Loss: 1.7764390684977298, Validation Loss: 1.759322274980091\n",
      "Epoch 15: Train Loss: 1.8368159923232903, Validation Loss: 2.2241853600456603\n",
      "Epoch 16: Train Loss: 1.9812109301671261, Validation Loss: 1.6814223779950823\n",
      "Epoch 17: Train Loss: 1.724949400605274, Validation Loss: 1.7449300336837767\n",
      "Epoch 18: Train Loss: 1.8784881125177655, Validation Loss: 1.6602625181561426\n",
      "Epoch 19: Train Loss: 1.6950110185046157, Validation Loss: 1.8551617398716154\n",
      "Epoch 20: Train Loss: 1.7168873604205477, Validation Loss: 1.7588567408493587\n",
      "Epoch 21: Train Loss: 1.7243664729495007, Validation Loss: 1.7145391264415923\n",
      "Epoch 22: Train Loss: 1.6905741663940814, Validation Loss: 1.8768468815939767\n",
      "Epoch 23: Train Loss: 1.6973059968988435, Validation Loss: 1.796612953912644\n",
      "Epoch 24: Train Loss: 1.800732716933018, Validation Loss: 1.5168900573821296\n",
      "Epoch 25: Train Loss: 1.7182925348722635, Validation Loss: 1.6386420664333161\n",
      "Epoch 26: Train Loss: 1.7051529684587687, Validation Loss: 1.716284388360523\n",
      "Epoch 27: Train Loss: 1.7033478399084396, Validation Loss: 1.688125840709323\n",
      "Epoch 28: Train Loss: 1.7696731618071804, Validation Loss: 1.7505443854559035\n",
      "Epoch 29: Train Loss: 1.8990981504897109, Validation Loss: 1.6832494874227615\n",
      "Epoch 30: Train Loss: 1.728497763641742, Validation Loss: 1.545569973446074\n",
      "Epoch 31: Train Loss: 1.9033668132589645, Validation Loss: 1.65145707039606\n",
      "Epoch 32: Train Loss: 1.654033415958661, Validation Loss: 1.9646265088944208\n",
      "Epoch 33: Train Loss: 1.7113557205280336, Validation Loss: 1.578468205134074\n",
      "Epoch 34: Train Loss: 1.799121505072137, Validation Loss: 2.321847018400828\n",
      "Epoch 35: Train Loss: 2.0121286230928757, Validation Loss: 1.7107941268739246\n",
      "Epoch 36: Train Loss: 1.7268856747010175, Validation Loss: 1.5290342646553403\n",
      "Epoch 37: Train Loss: 1.7781105605694427, Validation Loss: 1.7749464293888637\n",
      "Epoch 38: Train Loss: 1.7458257293300468, Validation Loss: 1.6700078898384458\n",
      "Epoch 39: Train Loss: 1.7039216914497504, Validation Loss: 1.6820199466886974\n",
      "Epoch 40: Train Loss: 1.854338299496835, Validation Loss: 2.497365253085182\n",
      "Epoch 41: Train Loss: 1.822844153292039, Validation Loss: 1.7136213507396834\n",
      "Epoch 42: Train Loss: 1.7270267824565664, Validation Loss: 1.6043935848417736\n",
      "Epoch 43: Train Loss: 1.8002172508359957, Validation Loss: 1.700114787760235\n",
      "Epoch 44: Train Loss: 1.7002491694338182, Validation Loss: 1.6796863485517957\n",
      "Epoch 45: Train Loss: 1.699011577357765, Validation Loss: 1.6984879339308965\n",
      "Epoch 46: Train Loss: 1.680481423630434, Validation Loss: 1.7660809139978317\n",
      "Epoch 47: Train Loss: 1.8554292965336006, Validation Loss: 1.7324447320756458\n",
      "Epoch 48: Train Loss: 1.706960900811588, Validation Loss: 1.7485376863252549\n",
      "Epoch 49: Train Loss: 1.697748200091995, Validation Loss: 1.650275901045118\n",
      "Epoch 50: Train Loss: 1.6895984283014507, Validation Loss: 1.7235489992868334\n",
      "Epoch 51: Train Loss: 1.6958245478357588, Validation Loss: 1.655040828614008\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 47\u001b[0m\n\u001b[1;32m     44\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     45\u001b[0m val_loader \u001b[38;5;241m=\u001b[39m DataLoader(val_dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 47\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m val_loss \u001b[38;5;241m=\u001b[39m validate(val_loader)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: Train Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Validation Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[37], line 9\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(train_loader)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m      8\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m----> 9\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_attr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(out, data\u001b[38;5;241m.\u001b[39my)\n\u001b[1;32m     11\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/td2c-qBinhRvk-py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/td2c-qBinhRvk-py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[34], line 19\u001b[0m, in \u001b[0;36mEdgeClassifier.forward\u001b[0;34m(self, x, edge_index, batch, edge_attr)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, edge_index, batch, edge_attr):\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;66;03m# Node feature learning\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     20\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x, edge_index)\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;66;03m# Edge feature concatenation (node features of source and target + original edge features)\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/td2c-qBinhRvk-py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/td2c-qBinhRvk-py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/td2c-qBinhRvk-py3.10/lib/python3.10/site-packages/torch_geometric/nn/conv/gcn_conv.py:241\u001b[0m, in \u001b[0;36mGCNConv.forward\u001b[0;34m(self, x, edge_index, edge_weight)\u001b[0m\n\u001b[1;32m    239\u001b[0m cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cached_edge_index\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 241\u001b[0m     edge_index, edge_weight \u001b[38;5;241m=\u001b[39m \u001b[43mgcn_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# yapf: disable\u001b[39;49;00m\n\u001b[1;32m    242\u001b[0m \u001b[43m        \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnode_dim\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimproved\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_self_loops\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcached:\n\u001b[1;32m    245\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cached_edge_index \u001b[38;5;241m=\u001b[39m (edge_index, edge_weight)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/td2c-qBinhRvk-py3.10/lib/python3.10/site-packages/torch_geometric/nn/conv/gcn_conv.py:110\u001b[0m, in \u001b[0;36mgcn_norm\u001b[0;34m(edge_index, edge_weight, num_nodes, improved, add_self_loops, flow, dtype)\u001b[0m\n\u001b[1;32m    108\u001b[0m deg \u001b[38;5;241m=\u001b[39m scatter(edge_weight, idx, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, dim_size\u001b[38;5;241m=\u001b[39mnum_nodes, reduce\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    109\u001b[0m deg_inv_sqrt \u001b[38;5;241m=\u001b[39m deg\u001b[38;5;241m.\u001b[39mpow_(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n\u001b[0;32m--> 110\u001b[0m \u001b[43mdeg_inv_sqrt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmasked_fill_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeg_inv_sqrt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m edge_weight \u001b[38;5;241m=\u001b[39m deg_inv_sqrt[row] \u001b[38;5;241m*\u001b[39m edge_weight \u001b[38;5;241m*\u001b[39m deg_inv_sqrt[col]\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m edge_index, edge_weight\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.BCELoss()\n",
    "\n",
    "def train_epoch(train_loader):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index, data.batch, data.edge_attr)\n",
    "        loss = criterion(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "\n",
    "def validate(val_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in val_loader:\n",
    "            out = model(data.x, data.edge_index, data.batch, data.edge_attr)\n",
    "            loss = criterion(out, data.y)\n",
    "            total_loss += loss.item() * data.num_graphs\n",
    "    return total_loss / len(val_loader.dataset)\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            out = model(data.x, data.edge_index, data.batch, data.edge_attr)\n",
    "            loss = criterion(out, data.y)\n",
    "            total_loss += loss.item() * data.num_graphs\n",
    "    return total_loss / len(test_loader.dataset)\n",
    "\n",
    "\n",
    "num_epochs = 200\n",
    "batch_size = 16\n",
    "for epoch in range(num_epochs):\n",
    "    # Reshuffle and split the dataset each epoch\n",
    "    train_size = int(0.85 * len(dataset))\n",
    "    train_dataset, val_dataset = random_split(dataset, [train_size, len(dataset) - train_size])\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    train_loss = train_epoch(train_loader)\n",
    "    val_loss = validate(val_loader)\n",
    "\n",
    "    print(f'Epoch {epoch+1}: Train Loss: {train_loss}, Validation Loss: {val_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "td2c-qBinhRvk-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
